{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels                  Id         label\n",
      "count  30336.000000  30336.000000\n",
      "mean   15167.500000      0.077664\n",
      "std     8757.393219      0.267646\n",
      "min        0.000000      0.000000\n",
      "25%     7583.750000      0.000000\n",
      "50%    15167.500000      0.000000\n",
      "75%    22751.250000      0.000000\n",
      "max    30335.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, BatchNormalization, Conv1D, Multiply, Activation, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "## Below path hardcoded. TODO: Change this\n",
    "prefix_path = '..'\n",
    "\n",
    "labels = pd.read_csv(prefix_path + '/train_kaggle.csv')\n",
    "\n",
    "print('Labels', labels.describe())\n",
    "\n",
    "\n",
    "## ones count kept to balance number of zeros and ones in data to be equal\n",
    "ones = len(labels.loc[labels['label']==1])\n",
    "\n",
    "max_len = 350\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape (5301, 350, 40)\n",
      "y shape (5301,)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SeparableConv1D\n",
    "## For each sample in the file\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for index, train_label in labels.iterrows():\n",
    "    label = train_label['label']\n",
    "    ## Checking below if number of zeros matches total number of ones, then stop adding zeros to data\n",
    "    if label == 0 and ones > 0:\n",
    "        ones = ones - 0.8\n",
    "    if ones <= 0 and label == 0:\n",
    "        continue\n",
    "    ## zeros_array used to keep the maximum number of sequences constant to max_len\n",
    "    zeros_array = np.zeros((max_len, 40))\n",
    "\n",
    "    ## features is a (N, 40) matrix\n",
    "    features = np.load(prefix_path + '/train/train/' + str(train_label['Id']) + '.npy')\n",
    "    \n",
    "    ## We add it to zeros_array to make all samples as (400, 40) matrix\n",
    "    zeros_array[0:len(features)] = features\n",
    "\n",
    "\n",
    "    ## For each feature, we find average of all values and replace all NaN with that value\n",
    "   # for feature in range(40):\n",
    "   #     average_value = np.average(zeros_array[:feature][np.nan_to_num(zeros_array[:feature]) != 0])\n",
    "   #     zeros_array[:feature] = np.nan_to_num(zeros_array[:feature], average_value)\n",
    "\n",
    "    X.append(zeros_array)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.nan_to_num(np.array(X))\n",
    "y = np.array(y)\n",
    "\n",
    "print('X Shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihir/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mihir/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihir/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'concatenate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1296925d6ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmax_pooling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_pooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#max_pooling = MaxPooling1D(pool_size=4)(data_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concatenate' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=4)\n",
    "\n",
    "##################################################################################\n",
    "data_input = Input(shape=(None, 40))\n",
    "\n",
    "normalize_input = BatchNormalization()(data_input)\n",
    "\n",
    "sig_conv = SeparableConv1D(40, (350, ), activation='sigmoid', padding='valid')(normalize_input)\n",
    "rel_conv1 = SeparableConv1D(40, (350, ), activation='tanh', padding='valid')(normalize_input)\n",
    "mul_conv = Multiply()([sig_conv, rel_conv1])\n",
    "\n",
    "#max_pooling = MaxPooling1D(pool_size=10)(mul_conv)\n",
    "#max_pooling = MaxPooling1D(pool_size=4)(data_input)\n",
    "\n",
    "lstm = Bidirectional(LSTM(64))(mul_conv)\n",
    "dropout = Dropout(0.4)(lstm)\n",
    "\n",
    "dense_1 = Dense(16, activation='relu')(dropout)\n",
    "dense_2 = Dense(1)(dropout)\n",
    "out1 = Activation('sigmoid')(dense_2)\n",
    "model1 = Model(input=data_input, output=out1)\n",
    "##################################################################################\n",
    "data_input = Input(shape=(None, 40))\n",
    "\n",
    "normalize_input = BatchNormalization()(data_input)\n",
    "\n",
    "sig_conv = Conv1D(20, (350, ), activation='sigmoid', padding='same')(normalize_input)\n",
    "rel_conv = Conv1D(20, (350, ), activation='tanh', padding='same')(normalize_input)\n",
    "mul_conv = Multiply()([sig_conv, rel_conv])\n",
    "\n",
    "lstm = Bidirectional(LSTM(64))(mul_conv)\n",
    "dropout = Dropout(0.4)(lstm)\n",
    "\n",
    "dense_1 = Dense(16, activation='relu')(dropout)\n",
    "dense_2 = Dense(1)(dropout)\n",
    "out2 = Activation('sigmoid')(dense_2)\n",
    "model2 = Model(input=data_input, output=out2)\n",
    "###################################################################################\n",
    "data_input = Input(shape=(None, 40))\n",
    "\n",
    "normalize_input = BatchNormalization()(data_input)\n",
    "\n",
    "sig_conv = SeparableConv1D(40, (1), activation='sigmoid', padding='valid')(normalize_input)\n",
    "rel_conv = SeparableConv1D(40, (1), activation='tanh', padding='valid')(normalize_input)\n",
    "mul_conv = Multiply()([sig_conv, rel_conv])\n",
    "\n",
    "max_pooling = MaxPooling1D(pool_size=2)(mul_conv)\n",
    "concat = concatenate([max_pooling, normalize_input])\n",
    "#max_pooling = MaxPooling1D(pool_size=4)(data_input)\n",
    "\n",
    "lstm = Bidirectional(LSTM(64))(mul_conv)\n",
    "dropout = Dropout(0.4)(lstm)\n",
    "\n",
    "dense_1 = Dense(16, activation='relu')(dropout)\n",
    "dense_2 = Dense(1)(dropout)\n",
    "out3 = Activation('sigmoid')(dense_2)\n",
    "model3 = Sequential(input=data_input, output=out3)\n",
    "####################################################################################\n",
    "models = [model1, model2, model3]\n",
    "outputs = [model.outputs[0] for model in models]\n",
    "y = Average()(outputs)\n",
    "\n",
    "modele = Model(data_input, y, name='ensemble')\n",
    "\n",
    "modele.compile(optimizer=Adam(lr=0.001), loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "print(modele.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4240 samples, validate on 1061 samples\n",
      "Epoch 1/100\n",
      "4240/4240 [==============================] - 4s 1ms/step - loss: 0.6695 - accuracy: 0.6311 - val_loss: 0.6094 - val_accuracy: 0.6956\n",
      "Epoch 2/100\n",
      "4240/4240 [==============================] - 4s 872us/step - loss: 0.6043 - accuracy: 0.6797 - val_loss: 0.5432 - val_accuracy: 0.7304\n",
      "Epoch 3/100\n",
      "4240/4240 [==============================] - 4s 965us/step - loss: 0.5197 - accuracy: 0.7278 - val_loss: 0.4682 - val_accuracy: 0.8106\n",
      "Epoch 4/100\n",
      "4240/4240 [==============================] - 4s 965us/step - loss: 0.4413 - accuracy: 0.8101 - val_loss: 0.4446 - val_accuracy: 0.8153\n",
      "Epoch 5/100\n",
      "4240/4240 [==============================] - 4s 995us/step - loss: 0.4018 - accuracy: 0.8151 - val_loss: 0.4216 - val_accuracy: 0.8209\n",
      "Epoch 6/100\n",
      "4240/4240 [==============================] - 4s 977us/step - loss: 0.3703 - accuracy: 0.8380 - val_loss: 0.4107 - val_accuracy: 0.8275\n",
      "Epoch 7/100\n",
      "4240/4240 [==============================] - 4s 970us/step - loss: 0.3503 - accuracy: 0.8476 - val_loss: 0.4154 - val_accuracy: 0.8172\n",
      "Epoch 8/100\n",
      "4240/4240 [==============================] - 4s 966us/step - loss: 0.3347 - accuracy: 0.8571 - val_loss: 0.4123 - val_accuracy: 0.8172\n",
      "Epoch 9/100\n",
      "4240/4240 [==============================] - 4s 962us/step - loss: 0.3288 - accuracy: 0.8561 - val_loss: 0.4048 - val_accuracy: 0.8238\n",
      "Epoch 10/100\n",
      "4240/4240 [==============================] - 4s 983us/step - loss: 0.3111 - accuracy: 0.8653 - val_loss: 0.3934 - val_accuracy: 0.8313\n",
      "Epoch 11/100\n",
      "4240/4240 [==============================] - 4s 956us/step - loss: 0.2937 - accuracy: 0.8745 - val_loss: 0.3867 - val_accuracy: 0.8275\n",
      "Epoch 12/100\n",
      "4240/4240 [==============================] - 4s 966us/step - loss: 0.2782 - accuracy: 0.8873 - val_loss: 0.3810 - val_accuracy: 0.8313\n",
      "Epoch 13/100\n",
      "4240/4240 [==============================] - 4s 968us/step - loss: 0.2693 - accuracy: 0.8903 - val_loss: 0.3820 - val_accuracy: 0.8322\n",
      "Epoch 14/100\n",
      "4240/4240 [==============================] - 4s 973us/step - loss: 0.2563 - accuracy: 0.8969 - val_loss: 0.3908 - val_accuracy: 0.8313\n",
      "Epoch 15/100\n",
      "4240/4240 [==============================] - 4s 967us/step - loss: 0.2434 - accuracy: 0.9014 - val_loss: 0.4055 - val_accuracy: 0.8266\n",
      "Epoch 16/100\n",
      "4240/4240 [==============================] - 4s 979us/step - loss: 0.2415 - accuracy: 0.9040 - val_loss: 0.3940 - val_accuracy: 0.8228\n",
      "Epoch 17/100\n",
      "4240/4240 [==============================] - 4s 963us/step - loss: 0.2243 - accuracy: 0.9083 - val_loss: 0.4045 - val_accuracy: 0.8228\n",
      "Epoch 18/100\n",
      "4240/4240 [==============================] - 4s 977us/step - loss: 0.2154 - accuracy: 0.9146 - val_loss: 0.4200 - val_accuracy: 0.8294\n",
      "Epoch 19/100\n",
      "4240/4240 [==============================] - 4s 999us/step - loss: 0.2041 - accuracy: 0.9182 - val_loss: 0.4536 - val_accuracy: 0.8294\n",
      "Epoch 20/100\n",
      "4240/4240 [==============================] - 4s 975us/step - loss: 0.1946 - accuracy: 0.9236 - val_loss: 0.4569 - val_accuracy: 0.8275\n",
      "Epoch 21/100\n",
      "4240/4240 [==============================] - 4s 984us/step - loss: 0.1841 - accuracy: 0.9302 - val_loss: 0.4775 - val_accuracy: 0.8294\n",
      "Epoch 22/100\n",
      "4240/4240 [==============================] - 4s 977us/step - loss: 0.1761 - accuracy: 0.9316 - val_loss: 0.4457 - val_accuracy: 0.8360\n",
      "Epoch 23/100\n",
      "4240/4240 [==============================] - 4s 969us/step - loss: 0.1707 - accuracy: 0.9370 - val_loss: 0.4828 - val_accuracy: 0.8313\n",
      "Epoch 24/100\n",
      "4240/4240 [==============================] - 4s 1ms/step - loss: 0.1574 - accuracy: 0.9373 - val_loss: 0.4451 - val_accuracy: 0.8473\n",
      "Epoch 25/100\n",
      "4240/4240 [==============================] - 4s 997us/step - loss: 0.1599 - accuracy: 0.9399 - val_loss: 0.5160 - val_accuracy: 0.8322\n",
      "Epoch 26/100\n",
      "4240/4240 [==============================] - 4s 973us/step - loss: 0.1547 - accuracy: 0.9413 - val_loss: 0.5872 - val_accuracy: 0.8332\n",
      "Epoch 27/100\n",
      "4240/4240 [==============================] - 4s 968us/step - loss: 0.1521 - accuracy: 0.9406 - val_loss: 0.4846 - val_accuracy: 0.8417\n",
      "Epoch 28/100\n",
      "4240/4240 [==============================] - 4s 977us/step - loss: 0.1417 - accuracy: 0.9469 - val_loss: 0.4894 - val_accuracy: 0.8435\n",
      "Epoch 29/100\n",
      "4240/4240 [==============================] - 4s 975us/step - loss: 0.1352 - accuracy: 0.9521 - val_loss: 0.5786 - val_accuracy: 0.8351\n",
      "Epoch 30/100\n",
      "4240/4240 [==============================] - 4s 1ms/step - loss: 0.1304 - accuracy: 0.9514 - val_loss: 0.5273 - val_accuracy: 0.8417\n",
      "Epoch 31/100\n",
      "4240/4240 [==============================] - 4s 1ms/step - loss: 0.1143 - accuracy: 0.9597 - val_loss: 0.5311 - val_accuracy: 0.8417\n",
      "Epoch 32/100\n",
      "4240/4240 [==============================] - 4s 1ms/step - loss: 0.1077 - accuracy: 0.9608 - val_loss: 0.5150 - val_accuracy: 0.8351\n",
      "Epoch 33/100\n",
      "4240/4240 [==============================] - 4s 978us/step - loss: 0.1016 - accuracy: 0.9684 - val_loss: 0.5563 - val_accuracy: 0.8388\n",
      "Epoch 34/100\n",
      "4240/4240 [==============================] - 4s 969us/step - loss: 0.1038 - accuracy: 0.9632 - val_loss: 0.6229 - val_accuracy: 0.8388\n",
      "Epoch 35/100\n",
      "4240/4240 [==============================] - 4s 982us/step - loss: 0.0952 - accuracy: 0.9665 - val_loss: 0.6287 - val_accuracy: 0.8351\n",
      "Epoch 36/100\n",
      "4240/4240 [==============================] - 4s 982us/step - loss: 0.0916 - accuracy: 0.9705 - val_loss: 0.6186 - val_accuracy: 0.8435\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-04a24a91c25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \t\t  validation_data=[x_test, y_test])\n\u001b[0m",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "\t\t  batch_size=batch_size,\n",
    "\t\t  epochs = 100,\n",
    "\t\t  validation_data=[x_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "ker = KerasRegressor(build_fn=get_keras_model,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4711, 40)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "KerasRegressor doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cdf1c24100a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAMME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5).fit(x_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# clf = svm.SVC(gamma='scale').fit(x_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Clear any previous fit results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[0;32m--> 445\u001b[0;31m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: KerasRegressor doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "X_2d = np.sum(X, axis=1)\n",
    "\n",
    "print(X_2d.shape)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_2d, y, test_size=0.20, random_state=4)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator= ker, n_estimators=100, algorithm='SAMME').fit(x_train, y_train)\n",
    "# clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5).fit(x_train, y_train)\n",
    "# clf = svm.SVC(gamma='scale').fit(x_train, y_train)\n",
    "# kernel = 1.0 * RBF(1.0)\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                    hidden_layer_sizes=(15, 10, 5), random_state=1).fit(x_train, y_train)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "score = pipeline.score(x_test, y_test)\n",
    "\n",
    "print('Score : ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihir/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/mihir/pythonprojects/cs5228_project/cs5228/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "test_X = []\n",
    "for fileno in range(10000):\n",
    "    ## zeros_array used to keep the maximum number of sequences constant to max_len\n",
    "    zeros_array = np.zeros((max_len, 40))\n",
    "\n",
    "    ## features is a (N, 40) matrix\n",
    "    features = np.load(prefix_path + '/test/test/' + str(fileno) + '.npy')\n",
    "    ## We add it to zeros_array to make all samples as (400, 40) matrix\n",
    "    zeros_array[0:len(features)] = features\n",
    "\n",
    "    ## For each feature, we find average of all values and replace all NaN with that value\n",
    "    for feature in range(40):\n",
    "        average_value = np.average(zeros_array[:feature][np.nan_to_num(zeros_array[:feature]) != 0])\n",
    "        zeros_array[:feature] = np.nan_to_num(zeros_array[:feature], average_value)\n",
    "\n",
    "    test_X.append(zeros_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40)\n",
      "(10000,)\n",
      "Ones  10000\n"
     ]
    }
   ],
   "source": [
    "test_X = np.nan_to_num(np.array(test_X))\n",
    "test_X_2d = np.sum(test_X, axis=1)\n",
    "\n",
    "print(test_X_2d.shape)\n",
    "\n",
    "print('Predicting test data')\n",
    "test_Y_2d = model.predict(np.nan_to_num(np.array(test_X)))\n",
    "\n",
    "#test_Y_2d = clf.predict(test_X_2d)\n",
    "print(test_Y_2d.shape)\n",
    "print('Ones ', np.count_nonzero(test_Y_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Id': np.array([i for i in range(10000)]), 'Predicted': test_Y_2d})\n",
    "\n",
    "df.to_csv('submission.csv', sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
